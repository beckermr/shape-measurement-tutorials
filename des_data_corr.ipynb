{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13a6ff05",
   "metadata": {},
   "source": [
    "# Measuring Background Pixel Correlations\n",
    "\n",
    "Let's build on the last tutorial and measure the correlation properties of the background pixels we've found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6041f640",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import fitsio\n",
    "import yaml\n",
    "import esutil\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ff3de5",
   "metadata": {},
   "source": [
    "## What is the correlation anyways?\n",
    "\n",
    "To get a handle on this, let's step back and talk about variance and covariance. Suppose we have two quantities, `A` and `B`. Formally in statistics, these would be called random variables, but for our purposes let's not worry about the jargon. We are casually familiar with the computing the mean. However, the formal definition of the mean is an integral like this\n",
    "\n",
    "$$\n",
    "E[A] = \\int dA\\, A P(A)\n",
    "$$\n",
    "\n",
    "The usual definition of the mean\n",
    "\n",
    "$$\n",
    "\\langle a\\rangle = \\frac{1}{N}\\sum a_i\n",
    "$$\n",
    "\n",
    "is known as an *estimator* of the mean. (Here the `a_i` are draws from the distribution $P(A)$. For example, if $P(A)$ is a Gaussian distribution, then the `a_i` would be a set of numbers whose histogram approaches the shape of a Gaussian as we increase the size of the set.) An estimator is something we compute from the data we observe as opposed to something defined by $P(A)$ like $E[A]$ above. It turns out that our estimator for the mean is unbiased since \n",
    "\n",
    "$$\n",
    "E[\\langle a \\rangle] = \\int da_0 ... \\int da_N\\, \\frac{1}{N}\\sum a_i P(a_0) ... P(a_N) = \\frac{1}{N}\\sum E[A] = E[A]\n",
    "$$\n",
    "\n",
    "However, this is not a general property of estimators. \n",
    "\n",
    "Now that we've define the mean, let's define the variance. This quantity is \n",
    "\n",
    "$$\n",
    "Var[A] = E[(A - E[A])^2] = \\int dA\\, (A - E[A])^2 P(A)\n",
    "$$\n",
    "\n",
    "The standard deviation is simply $Std[A] = \\sqrt{Var[A]}$. There are again estimators for the standard deviation. You are likely familiar with the standard one with a factor of N-1 in the denominator. We won't go through the math here, but this odd looking N-1 is needed to make the estimator an unbiased estimate of the standard deviation.\n",
    "\n",
    "Finally, we can define the covariance\n",
    "\n",
    "$$\n",
    "Cov[A, B] = E[(A-E[A])(B-E[B])] = \\int dAdB\\, (A-E[A])(B-E[B]) P(A, B) \n",
    "$$\n",
    "\n",
    "where now this is a double integral and we have the joint distribution of $A$ and $B$, $P(A,B)$.\n",
    "\n",
    "### Exercise 1\n",
    "\n",
    "Compute $Cov[A,A]$. Do you recognize this quantity? What is the general property or rule here?\n",
    "\n",
    "## Correlation, finally.\n",
    "\n",
    "Now that we understand the covariance, the correlation is defined as\n",
    "\n",
    "$$\n",
    "Corr[A, B] = \\frac{Cov[A,B]}{Std[A]\\,Std[B]}\n",
    "$$\n",
    "\n",
    "The correlation has a nice property that it is always bounded between between -1 and 1. Positive values are indicate correlation (i.e., as A goes up, B goes up too) and negative values indicate anticorrelation (i.e., as A goes up, B goes down).\n",
    "\n",
    "## Measuring the correlation of pixels in an image\n",
    "\n",
    "For an image, the concept of correlation is a bit more complicated. Instead of two sets of values A and B (e.g., height and age or something), we a full image worth of pixels. Further, we might have more than one image to handle. To handle these issues, we need to make some definitions. First, we're going to measure the correlation of pixels separated by some distance, say one pixel. The idea here is that we are asking the question, if a given pixel goes up, do the pixels next to it go up or down? (By up and down here I really mean bigger or less than the mean.)\n",
    "\n",
    "In math, we have for pixel value $V_ij$ and a separation of one pixel spacing in $j$ a quantity like\n",
    "\n",
    "$$\n",
    "C_{i,j+1} = \\langle Cov[V_{i,j}, V_{i,j+1}]\\rangle\n",
    "$$\n",
    "\n",
    "where the average is over all possible values of $i,j$. We can of course measure this for any offset in $\\Delta i,\\Delta j$. It is helpful to arrange these values into a 3x3 matrix where the center element is the above quantity with the offsets at zero and then each outer element is the quantity above with the offset given by the offset to the location in the matrix. Visually we have a grid of values like this\n",
    "\n",
    "$$\n",
    "\\begin{array}{ccc}\n",
    "C_{i-1,j+1} & C_{i,j+1} & C_{i+1,j+1}\\\\\n",
    "C_{i-1,j} & C_{i,j} & C_{i+1,j}\\\\\n",
    "C_{i-1,j-1} & C_{i,j-1} & C_{i+1,j-1}\\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "Finally, in our case, we have pixels which we want to ignore and so we cannot simply use all of the pixels we find. I have put a function below that does the computation we'd like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f8a45705",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numba\n",
    "\n",
    "@numba.njit()\n",
    "def _meas_cov(im, msk, cov, nrm):\n",
    "    for ic in range(im.shape[0]):\n",
    "        io_min = max(ic-1, 0)\n",
    "        io_max = min(ic+1, im.shape[0]-1) + 1\n",
    "        \n",
    "        for jc in range(im.shape[1]):\n",
    "            jo_min = max(jc-1, 0)\n",
    "            jo_max = min(jc+1, im.shape[1]-1) + 1\n",
    "            \n",
    "            if not msk[ic, jc]:\n",
    "                continue\n",
    "            \n",
    "            for io in range(io_min, io_max):\n",
    "                i = io - ic + 1\n",
    "                \n",
    "                for jo in range(jo_min, jo_max):\n",
    "                    j = jo - jc + 1\n",
    "                    \n",
    "                    if not msk[io, jo]:\n",
    "                        continue\n",
    "\n",
    "                    cov[i, j] += (im[ic, jc] * im[io, jo])\n",
    "                    nrm[i, j] += 1\n",
    "                    \n",
    "    for i in range(3):\n",
    "        for j in range(3):\n",
    "            cov[i, j] /= nrm[i, j]\n",
    "\n",
    "\n",
    "def meas_cov(im, msk):\n",
    "    \"\"\"Measure the one-offset covariance of pixels in an image, ignoring bad ones.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    im : np.ndarray\n",
    "        The image.\n",
    "    msk : np.ndarray\n",
    "        The mask where True is a good pixel and False is a bad pixel.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    cov : np.ndarray\n",
    "        The 3x3 matrix of covariances between the pixels.\n",
    "    \"\"\"\n",
    "    cov = np.zeros((3, 3))\n",
    "    nrm = np.zeros((3, 3))\n",
    "    mn = np.mean(im[msk])\n",
    "    \n",
    "    _meas_cov(im.astype(np.float64)-mn, msk, cov, nrm)\n",
    "    return cov"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc68854",
   "metadata": {},
   "source": [
    "### Exercise 2: Measure covariance matrix of Gaussian random image\n",
    "\n",
    "1. Make an image of Gaussian random draws and measure the covariance matrix. I have put some code below to get you started.\n",
    "2. Try different sizes (e.g., 10x10, 100x100, 1000x1000). What do you notice about the results in relationship to the correct answer as you increase the image size? What is the correct answer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a3a4bfca",
   "metadata": {},
   "outputs": [],
   "source": [
    "im = np.random.normal(size=(1000, 1000))\n",
    "msk = np.ones_like(im)\n",
    "\n",
    "# use the function above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642dcb19",
   "metadata": {},
   "source": [
    "## Exercise 3: Measure the covariance matrix for a single epoch image\n",
    "\n",
    "Now let's move on to a single epoch image. I have put a bunch of code below to help in making the cuts etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d0e4b53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_wcs(pth, ext=0):\n",
    "    hdr = fitsio.read_header(pth, ext=ext)\n",
    "    dct = {}\n",
    "    for k in hdr.keys():\n",
    "        try:\n",
    "            dct[k.lower()] = hdr[k]\n",
    "        except Exception:\n",
    "            pass\n",
    "    return esutil.wcsutil.WCS(dct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f8902d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "meds_dir = \"/cosmo/scratch/mrbecker/MEDS_DIR\"\n",
    "tilename = \"DES0124-3332\"\n",
    "band = \"i\"\n",
    "yaml_pth = os.path.join(\n",
    "    meds_dir, \n",
    "    \"des-pizza-slices-y6-v8/pizza_cutter_info/%s_%s_pizza_cutter_info.yaml\" % (\n",
    "        tilename, band\n",
    "    )\n",
    ")\n",
    "\n",
    "with open(yaml_pth, \"r\") as fp:\n",
    "    info = yaml.safe_load(fp.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "90c7c50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "coadd_wcs = read_wcs(info['image_path'], ext=info['image_ext'])\n",
    "coadd_image = fitsio.read(info['image_path'], ext=info['image_ext'])\n",
    "coadd_weight = fitsio.read(info['weight_path'], ext=info['weight_ext'])\n",
    "coadd_bmask = fitsio.read(info['bmask_path'], ext=info['bmask_ext'])\n",
    "coadd_seg = fitsio.read(info['seg_path'], ext=info['seg_ext'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5b8717e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mask_and_image(si, coadd_wcs, coadd_weight, coadd_bmask, coadd_seg):\n",
    "    # read all of the data here\n",
    "    se_wcs = read_wcs(si['image_path'], ext=si['image_ext'])\n",
    "    se_image = fitsio.read(si['image_path'], ext=si['image_ext'])\n",
    "    se_weight = fitsio.read(si['weight_path'], ext=si['weight_ext'])\n",
    "    se_bmask = fitsio.read(si['bmask_path'], ext=si['bmask_ext'])\n",
    "    se_bkg = fitsio.read(si['bkg_path'], ext=si['bkg_ext'])\n",
    "    se_image -= se_bkg\n",
    "    se_image *= si['scale']\n",
    "    \n",
    "    # handle the WCS transforms\n",
    "    xind, yind = np.meshgrid(np.arange(se_image.shape[1]), np.arange(se_image.shape[0]))\n",
    "    xind = xind.ravel()\n",
    "    yind = yind.ravel()\n",
    "\n",
    "    # map to ra, dec\n",
    "    ra, dec = se_wcs.image2sky(x=xind+1, y=yind+1)\n",
    "\n",
    "    # now map back to coadd pixel coordinates\n",
    "    x_c, y_c = coadd_wcs.sky2image(ra, dec)\n",
    "\n",
    "    # now take nearest pixel\n",
    "    x_c = (x_c + 0.5).astype(np.int32)\n",
    "    y_c = (y_c + 0.5).astype(np.int32)\n",
    "\n",
    "    # finally, to get back to the indices into a coadd image, we need to subtract \n",
    "    # one per our discussion above\n",
    "    x_i = x_c - 1\n",
    "    y_i = y_c - 1\n",
    "    \n",
    "    # record unphysical rows and cols\n",
    "    pmsk = (\n",
    "        (x_i >= 0)\n",
    "        & (x_i < 10000)\n",
    "        & (y_i >= 0)\n",
    "        & (y_i < 10000)\n",
    "    ).reshape(se_image.shape)\n",
    "    \n",
    "    # now clip to proper range\n",
    "    x_i = np.clip(x_i, 0, 9999)\n",
    "    y_i = np.clip(y_i, 0, 9999)\n",
    "    \n",
    "    # now grab values\n",
    "    se_coadd_seg = coadd_seg[y_i, x_i].reshape(se_image.shape)\n",
    "    se_coadd_bmask = coadd_bmask[y_i, x_i].reshape(se_image.shape)\n",
    "    se_coadd_weight = coadd_weight[y_i, x_i].reshape(se_image.shape)    \n",
    "    \n",
    "    \n",
    "    msk = (\n",
    "        (se_coadd_seg == 0)      # selects sky pixels\n",
    "        & (se_coadd_bmask == 0)  # selects pixels without defects\n",
    "        & (se_bmask == 0)\n",
    "        & (se_coadd_weight > 0)  # selects pixels with finite variance\n",
    "        & (se_weight > 0)\n",
    "        & pmsk                   # select SE pixels that actuall fell onto the coadd image\n",
    "    )    \n",
    "    \n",
    "    return msk, se_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4ea7c1",
   "metadata": {},
   "source": [
    "Your task is loop through all of the SE images for the coadd tile (each entry in `info['src_info']`) and measure the covariance matrices. Scale each one by the variance to compute the correlation matrix. Record them in a list. Then come up with some way to display the results in a plot that shows how different they are.\n",
    "\n",
    "The code snippet below will measure the covariance matrix for a single entry of the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f39fffff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00274254 0.00329531 0.00263506]\n",
      " [0.00277856 1.         0.00277856]\n",
      " [0.00263506 0.00329531 0.00274254]]\n"
     ]
    }
   ],
   "source": [
    "se_ind = 5  # 5th image is index 4\n",
    "si = info['src_info'][se_ind]\n",
    "\n",
    "msk, im = get_image_mask(si, coadd_wcs, coadd_weight, coadd_bmask, coadd_seg)\n",
    "cov = meas_cov(im, msk)\n",
    "corr = cov / cov[1, 1]\n",
    "print(corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1399e263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is the skeleton of a loop to get you started\n",
    "results = []\n",
    "for i in range(len(info['src_info'])):\n",
    "    si = info['src_info'][i]\n",
    "\n",
    "    results.append(corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6319b2",
   "metadata": {},
   "source": [
    "## Save the Data\n",
    "\n",
    "You can use python pickle to save the data for now. If your data is in the python variable `results`, then this looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb14fc84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# write the data\n",
    "with open(\"test.pkl\", \"wb\") as fp:\n",
    "    pickle.dump(results, fp)\n",
    "\n",
    "# read the data\n",
    "with open(\"test.pkl\", \"rb\") as fp:\n",
    "    new_res = pickle.load(fp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:des-pixcorr] *",
   "language": "python",
   "name": "conda-env-des-pixcorr-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
